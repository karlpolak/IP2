{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') # hide warnings\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from pickle import dump\n",
    "from dyntapy.supply_data import get_toy_network, relabel_graph\n",
    "from dyntapy.demand_data import add_centroids, od_graph_from_matrix\n",
    "from dyntapy.visualization import show_network, show_demand\n",
    "from dyntapy.assignments import StaticAssignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# settings\n",
    "city = \"BRUSSEL\"\n",
    "radius1 = \"10-20\"\n",
    "radius2 = \"20-40\"\n",
    "groupSize1 = 4\n",
    "groupSize2 = 16\n",
    "\n",
    "# TODO make code more waterproof (naming of input and outputfile)\n",
    "def aggregate_zones(city = city, radius = radius1, idealNbZonesPerCluster = groupSize1):\n",
    "    '''\n",
    "    original shapefile will change: 'cluster' column containing the cluster to which each zone belongs\n",
    "    new shapefill will be created: shapefile consisting of aggregated zones\n",
    "\n",
    "    Note:\n",
    "    code not waterproof: always looks in folder \"STA_prep/shapefile_data/{city}_40_10/\"\n",
    "    '''\n",
    "    inputFile = f\"STA_prep/shapefile_data/{city}_40_10/{city}_40_10_{radius}.shp\"\n",
    "    outputFile = f\"STA_prep/shapefile_data/{city}_40_10/{city}_40_10_{radius}_knn.shp\"\n",
    "    shapefile = gpd.read_file(inputFile)\n",
    "\n",
    "    points = np.array(shapefile.centroid.apply(lambda p: [p.x, p.y]).tolist())\n",
    "\n",
    "    k =  int(np.ceil(len(shapefile) / idealNbZonesPerCluster))  # number of clusters to create\n",
    "    print(f'Aggregation of {len(shapefile)} zones into {k} clusters.')\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0).fit(points)\n",
    "\n",
    "    shapefile[\"cluster\"] = kmeans.labels_\n",
    "    aggregated = shapefile.dissolve(by=\"cluster\").reset_index()\n",
    "    basicStatisticsZoneSizes = shapefile.groupby('cluster').size().describe() # contains\n",
    "    print(f\"Basic statistics of zone aggregation:\\n{basicStatisticsZoneSizes}\\n\")\n",
    "\n",
    "\n",
    "    # Save the new shapefile to a file\n",
    "    shapefile.to_file(inputFile)\n",
    "    aggregated.to_file(outputFile)\n",
    "\n",
    "    # Return aggregated shapefile\n",
    "    return aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_shapefile1 = aggregate_zones()\n",
    "agg_shapefile2 = aggregate_zones(radius = radius2, idealNbZonesPerCluster = groupSize2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After-market fix: add 'cluster' column to unclustered 0-10 radius such that later we have a cluster value for all zones in radius 0-40\n",
    "shapefile0 = gpd.read_file(f'STA_prep/shapefile_data/{city}_40_10/{city}_40_10_0-10.shp')\n",
    "shapefile0['cluster'] = range(shapefile0.shape[0])\n",
    "shapefile0.to_file(f'STA_prep/shapefile_data/{city}_40_10/{city}_40_10_0-10_knn.shp') # no k-nearest neighbour, but just for naming convention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate shapefiles\n",
    "import pandas as pd\n",
    "shapefile1 = gpd.read_file(f'STA_prep/shapefile_data/{city}_40_10/{city}_40_10_0-10_knn.shp')\n",
    "shapefile2 = gpd.read_file(f'STA_prep/shapefile_data/{city}_40_10/{city}_40_10_{radius1}_knn.shp')\n",
    "shapefile3 = gpd.read_file(f'STA_prep/shapefile_data/{city}_40_10/{city}_40_10_{radius2}_knn.shp')\n",
    "\n",
    "# add scalar to cluster numbers before concatenating shapefiles into combined shapefile\n",
    "shapefile2['cluster'] = shapefile2['cluster'] + (shapefile1.cluster.max()+1)\n",
    "shapefile3['cluster'] = shapefile3['cluster'] + (shapefile2.cluster.max()+1)\n",
    "\n",
    "# combine aggregated shapefiles into combined shapefile\n",
    "combined_shapefile = gpd.GeoDataFrame(pd.concat([shapefile1, shapefile2, shapefile3]))\n",
    "combined_shapefile.to_file('STA_prep/shapefile_data/BRUSSEL_40_10/BRUSSEL_40_10_aggr_comb.shp')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try OD matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OD matrix\n",
    "original_od_matrix = pd.read_excel(\"STA_prep/od_matrix_data/BRUSSEL_40_9_.xlsx\") # TODO define dynamically\n",
    "\n",
    "# mean geometry and summed demand in each cluster\n",
    "# clustered_zones = combined_shapefile.groupby(\"cluster\").agg({\"geometry\": \"mean\", \"demand\": \"sum\"})\n",
    "\n",
    "# Create a new origin-destination matrix with aggregated zones\n",
    "nbAggZones = len(combined_shapefile.groupby(\"cluster\"))\n",
    "aggregated_od_matrix = np.zeros((nbAggZones, nbAggZones))\n",
    "\n",
    "# for i in range(nbAggZones):\n",
    "#     for j in range(nbAggZones):\n",
    "#         # # TODO: find indices of original zones belonging to current aggregated zones \n",
    "#         # # (problem: clusters are number 0 .. nbClusters - 1 for all radiuses, so numbers will occur multiple times, even though they belong to different regions)\n",
    "#         # zones_i = combined_shapefile[combined_shapefile[\"cluster\"] == i]\n",
    "#         # zones_j = combined_shapefile[combined_shapefile[\"cluster\"] == j]\n",
    "#         # indices_i = np.where(combined_shapefile[\"cluster\"] == i) # TODO use these indices to retrieve ZONENUMMER from the correct rows\n",
    "#         # indices_j = np.where(combined_shapefile[\"cluster\"] == j) \n",
    "        \n",
    "#         # sum the demand between all pairs of original zones that belong to the current aggregated zones\n",
    "#         demand_sum = 0\n",
    "#         for origZone_i in indices_i:\n",
    "#             for origZone_j in indices_j:\n",
    "#                 demand_sum += original_od_matrix[origZone_i, origZone_j]\n",
    "        \n",
    "#         # assign the demand sum to the current cell in the aggregated OD matrix\n",
    "#         aggregated_od_matrix[i, j] = demand_sum\n",
    "\n",
    "\n",
    "# # Save the new origin-destination matrix to an excel file\n",
    "# aggregated_od_matrix.to_excel(\"STA_prep/od_matrix_data/BRUSSEL_40_9_aggregated.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_od_matrix.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "j = 2\n",
    "\n",
    "zones_i = combined_shapefile[combined_shapefile[\"cluster\"] == i]\n",
    "zones_j = combined_shapefile[combined_shapefile[\"cluster\"] == j]\n",
    "i_indices = np.where(combined_shapefile[\"cluster\"] == i)\n",
    "j_indices = np.where(combined_shapefile[\"cluster\"] == j)\n",
    "print(zones_i)\n",
    "print(zones_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dyntapy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
