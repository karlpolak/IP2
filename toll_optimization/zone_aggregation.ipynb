{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zone aggregation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. At first, you import all the necessary packages and files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') # hide warnings\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Short overview on the next steps given below!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please be aware that the numbering starts at 2 to be compliant with the notebook numbering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OVERVIEW:\n",
    "# Step 2: Relabel the zonenumbers to our actual numbers (such that zone 3 has id 3, not id 7 for example)\n",
    "# Step 3: Split the shapefile into the different regions that will experience different degrees of aggregations\n",
    "# Step 4: Clustering + retrieving for each original zone to which cluster it belongs\n",
    "# Step 5: Update the centroids and the OD-matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Relabel the zonenumbers to the actual numbers (such that zone 3 has id 3, not id 7 for example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2\n",
    "city = \"BRUSSEL\"\n",
    "radius = \"40\"\n",
    "\n",
    "path_orig_shapefile = f\"data_map/STA/shapefiles/{city}_{radius}_10/{city}_{radius}_10.shp\"\n",
    "qgis_path = f\"data_map/QGIS/{city}_{radius}_10.shp\"\n",
    "shapefile = gpd.read_file(path_orig_shapefile)\n",
    "shapefile[\"ZONENUMMER\"] = list(range(1,len(shapefile)+1))\n",
    "shapefile.to_file(qgis_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. This step is done in QGIS. Split the shapefile into the different regions that will experience different degrees of aggregations, in the example case this is chosen to be based on different radiusses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: QGIS\n",
    "'''\n",
    "The main idea here is to determine the different areas around the area of interest that will experience a different degree of aggregation.\n",
    "Here, it is decided that:\n",
    "    * a radius of 10km around Brussels remains unclustered\n",
    "    * a radius from 10 to 20 km is aggregated into somewhat larger zones\n",
    "    * a radius from 20 to 40 km is aggregated into even larger zones\n",
    "For this, the entire region is divided into bands around the center.\n",
    "\n",
    "Step-by-step guide for QGIS:\n",
    "    1. Find centroid of entire region of impact \n",
    "        OUR CASE: \n",
    "            toll_optimization/data_map/QGIS/radius_and_center/center.shp\n",
    "\n",
    "    2. Create circle with 45km radius around centroid. 45km instead of the actual radius of 40km of the area to make sure the most outer zones fall into the created circle.\n",
    "\n",
    "    3. Split circle into rings with radiuses of 0-10km (circle to be exact), 10-20km and 20-40km. \n",
    "        e.g. by creating smaller circles and taking differences of the right combinations of circles.\n",
    "        OUR CASE:\n",
    "            toll_optimization/data_map/QGIS/radius_and_center/0-10.shp\n",
    "            toll_optimization/data_map/QGIS/radius_and_center/10-20.shp\n",
    "            toll_optimization/data_map/QGIS/radius_and_center/20-40.shp\n",
    "\n",
    "    4. For each ring, select the zones of the entire region that are located within the ring. \n",
    "        Hint: use the \"select within\" plugin. There the location is unambiguously decided on the centroid of the zone\n",
    "        OUR CASE:\n",
    "            toll_optimization/data_map/QGIS/BRUSSEL_40_10_0-10.shp\n",
    "            toll_optimization/data_map/QGIS/BRUSSEL_40_10_10-20.shp\n",
    "            toll_optimization/data_map/QGIS/BRUSSEL_40_10_20-40.shp\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Clustering step and afterwards also retrieving for each original zone to which cluster it belongs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: clustering\n",
    "\n",
    "# settings\n",
    "radius1 = \"0-10\"\n",
    "radius2 = \"10-20\"\n",
    "radius3 = \"20-40\"\n",
    "# groupSize1 = 1 not needed\n",
    "groupSize2 = 4\n",
    "groupSize3 = 16\n",
    "\n",
    "def aggregate_zones(city = city, radius = radius2, idealNbZonesPerCluster = groupSize2):\n",
    "    '''\n",
    "    original shapefile will change: 'cluster' column containing the cluster to which each zone belongs\n",
    "    new shapefill will be created: shapefile consisting of aggregated zones\n",
    "    '''\n",
    "    inputFile = f\"data_map/QGIS/{city}_40_10_{radius}.shp\"\n",
    "    outputFile = f\"data_map/QGIS/{city}_40_10_{radius}_knn.shp\"\n",
    "    shapefile = gpd.read_file(inputFile)\n",
    "\n",
    "    points = np.array(shapefile.centroid.apply(lambda p: [p.x, p.y]).tolist())\n",
    "\n",
    "    k =  int(np.ceil(len(shapefile) / idealNbZonesPerCluster))  # number of clusters to create\n",
    "    print(f'Aggregation of {len(shapefile)} zones into {k} clusters.')\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0).fit(points) \n",
    "\n",
    "    shapefile[\"cluster\"] = kmeans.labels_\n",
    "    aggregated = shapefile.dissolve(by=\"cluster\").reset_index()\n",
    "    basicStatisticsZoneSizes = shapefile.groupby('cluster').size().describe() # contains\n",
    "    print(f\"Basic statistics of zone aggregation:\\n{basicStatisticsZoneSizes}\\n\")\n",
    "\n",
    "    # Create a new attribute column to store which elements each cluster contains\n",
    "    aggregated[\"elements_1\"] = \"\"\n",
    "    aggregated[\"elements_2\"] = \"\"\n",
    "\n",
    "    # Retrieve which zones are clustered into which cluster\n",
    "    cluster_groups = shapefile.groupby(\"cluster\")\n",
    "    cluster_elements = {}\n",
    "    for cluster_id, group in cluster_groups:\n",
    "        element_ids = group[\"ZONENUMMER\"].tolist()\n",
    "        cluster_elements[cluster_id] = element_ids\n",
    "\n",
    "    # Update the attribute column with the zonenummers for each cluster. Since the attribute columns are limited in size, \n",
    "    # a second column is used if more than 40 zones are clustered into one aggregated zone. \n",
    "    for index, row in aggregated.iterrows():\n",
    "        cluster_id = row[\"cluster\"]\n",
    "        element_ids = cluster_elements.get(cluster_id, [])\n",
    "        if len(element_ids) > 40:\n",
    "            element_ids1 = element_ids[0:40]\n",
    "            element_ids2 = element_ids[40:]\n",
    "            aggregated.at[index, \"elements_1\"] = \", \".join(str(e) for e in element_ids1)\n",
    "            aggregated.at[index, \"elements_2\"] = \", \".join(str(e) for e in element_ids2)\n",
    "        else: \n",
    "            aggregated.at[index, \"elements_1\"] = \", \".join(str(e) for e in element_ids)\n",
    "    \n",
    "    # Save the new shapefile to a file & update the original file\n",
    "    shapefile.to_file(inputFile)\n",
    "    aggregated.to_file(outputFile)\n",
    "\n",
    "    # Return aggregated shapefile\n",
    "    return aggregated\n",
    "\n",
    "agg_shapefile1 = aggregate_zones()\n",
    "agg_shapefile2 = aggregate_zones(radius = radius3, idealNbZonesPerCluster = groupSize3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Update the centroids and the OD-matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5: Update centroids & OD. First we do some processing to make things easier. \n",
    "\n",
    "# Quick fix: add 'cluster' column to unclustered 0-10 radius such that later we have a cluster value for all zones in radius 0-40\n",
    "# no k-nearest neighbour is ran on the smallest radius, but just for naming convention some processing is done \n",
    "shapefile0 = gpd.read_file(f'data_map/QGIS/{city}_{radius}_10_{radius1}.shp')\n",
    "shapefile0['cluster'] = range(shapefile0.shape[0])\n",
    "shapefile0['elements_1'] = shapefile0['ZONENUMMER']\n",
    "shapefile0.to_file(f'data_map/QGIS/{city}_{radius}_10_{radius1}_knn.shp') \n",
    "\n",
    "# Concatenate shapefiles\n",
    "shapefile1 = gpd.read_file(f'data_map/QGIS/{city}_{radius}_10_{radius1}_knn.shp')\n",
    "shapefile2 = gpd.read_file(f'data_map/QGIS/{city}_{radius}_10_{radius2}_knn.shp')\n",
    "shapefile3 = gpd.read_file(f'data_map/QGIS/{city}_{radius}_10_{radius3}_knn.shp')\n",
    "\n",
    "# add scalar to cluster numbers before concatenating shapefiles into combined shapefile (otherwise we start counting from 0 multiple times in the combined shapefile)\n",
    "shapefile2['cluster'] = shapefile2['cluster'] + (shapefile1.cluster.max()+1)\n",
    "shapefile3['cluster'] = shapefile3['cluster'] + (shapefile2.cluster.max()+1)\n",
    "\n",
    "# Save combined shapefile\n",
    "combined_shapefile = gpd.GeoDataFrame(pd.concat([shapefile1, shapefile2, shapefile3]))\n",
    "combined_shapefile.to_file(f'data_map/QGIS/{city}_{radius}_10_aggr_comb.shp')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`The final zoning shapefile, containing the aggregated shapefiles all combined, is thus available at: toll_optimization/data_map/QGIS/BRUSSEL_40_10_aggr_comb.shp`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Retrieve original OD matrix, centroids and aggregated shapefile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_od_matrix = pd.read_excel(f\"data_map/STA/OD_matrix/{city}_{radius}_9_.xlsx\") \n",
    "combined_shapefile = gpd.read_file(f'data_map/QGIS/{city}_{radius}_10_aggr_comb.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize new OD\n",
    "nbAggZones = len(combined_shapefile)\n",
    "aggregated_od_matrix = np.zeros((nbAggZones, nbAggZones))\n",
    "\n",
    "# Retrieve combined zones of each cluster \n",
    "zones_per_cluster = []\n",
    "for index, row in combined_shapefile.iterrows():\n",
    "    zones_cluster_str = row['elements_1']\n",
    "    zones_cluster = [int(e) for e in zones_cluster_str.split(\",\")]\n",
    "    if len(zones_cluster) >= 40:\n",
    "        zones_cluster_str = row['elements_2']\n",
    "        more_zones = [int(e) for e in zones_cluster_str.split(\",\")]\n",
    "        for zone in more_zones:\n",
    "            zones_cluster.append(zone)\n",
    "    zones_per_cluster.append(zones_cluster) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified OD matrix\n",
    "# Outer loop\n",
    "for index_i, row_i in combined_shapefile.iterrows():\n",
    "    # Retrieve all the zones belonging to cluster i    \n",
    "    zones_cluster_i = zones_per_cluster[index_i]\n",
    "    # Inner loop\n",
    "    for index_j, row_j in combined_shapefile.iterrows():\n",
    "        # Retrieve all the zones belonging to cluster j \n",
    "        zones_cluster_j = zones_per_cluster[index_j]\n",
    "        \n",
    "        # Calculate OD flows: sum up the individual flows\n",
    "        aggr_od_flow = 0\n",
    "        for zone_i in zones_cluster_i:\n",
    "            for zone_j in zones_cluster_j:\n",
    "                # VERY IMPORTANT: PANDAS INDEXING SHOULD FIRST SPECIFY COLUMN, AND THEN THE ROW\n",
    "                # Otherwise we are flipping the direction of the summed flows... \n",
    "                aggr_od_flow += original_od_matrix[zone_j-1][zone_i-1]\n",
    "        aggregated_od_matrix[index_i][index_j] = aggr_od_flow\n",
    "\n",
    "# Remove zones that have no demand at all (internal demand is ignored) from the shapefile and the OD_matrix. \n",
    "indices_zero_rows = np.where((aggregated_od_matrix==0).all(axis=1))[0]\n",
    "indices_zero_cols = np.where((aggregated_od_matrix==0).all(axis=0))[0]\n",
    "aggregated_od_matrix = np.delete(np.delete(aggregated_od_matrix, indices_zero_cols, axis=0), indices_zero_rows, axis=1)\n",
    "aggregated_od_matrix = pd.DataFrame(aggregated_od_matrix)\n",
    "combined_shapefile.drop(indices_zero_rows, inplace=True) # Remove entries without creating new geodataframe\n",
    "combined_shapefile[\"cluster\"] = list(range(0,len(combined_shapefile)))\n",
    "\n",
    "# Check: no rows or columns are completely filled with zeroes. --> Prints should be empty\n",
    "indices_zero_rows = np.where((aggregated_od_matrix==0).all(axis=1))[0]\n",
    "indices_zero_cols = np.where((aggregated_od_matrix==0).all(axis=0))[0]\n",
    "print(indices_zero_cols)\n",
    "print(indices_zero_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified centroids\n",
    "import typing\n",
    "from shapely.geometry import Polygon\n",
    "polygons: typing.List[Polygon] = combined_shapefile[\"geometry\"]\n",
    "cluster_ids = combined_shapefile[\"cluster\"]\n",
    "\n",
    "points = [polygon.centroid for polygon in polygons]\n",
    "centroids = []\n",
    "centroid_x = []\n",
    "centroid_y = []\n",
    "for cluster, point in zip(cluster_ids, points):\n",
    "    centroids.append({'cluster_id': cluster, 'X_LAMB': point.x, 'Y_LAMB': point.y})\n",
    "    centroid_x.append(point.x)\n",
    "    centroid_y.append(point.y)\n",
    "combined_shapefile[\"X_LAMB\"] = centroid_x\n",
    "combined_shapefile[\"Y_LAMB\"] = centroid_y\n",
    "centroids_shapefile = gpd.GeoDataFrame(centroids, geometry=points)\n",
    "centroids_shapefile.crs = {'init': combined_shapefile.crs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update centroids in the shapefile\n",
    "combined_shapefile.to_file(f'data_map/QGIS/{city}_{radius}_10_aggr_comb.shp')\n",
    "# Save the centroids to a shapefile, such that they can be added as a layer\n",
    "centroids_shapefile.to_file(f'data_map/QGIS/centroids_{city}_{radius}_aggr_comb.shp')\n",
    "# Save new OD flows to excel\n",
    "aggregated_od_matrix.to_excel(f\"data_map/STA/OD_matrix/{city}_{radius}_9_aggregated.xlsx\", index=False) # Index is False removes the index column (which matches the format of the original OD matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Congratulations, you have aggregated small zones into larger ones and are set to start assignments on bigger networks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dyntapy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
