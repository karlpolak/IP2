{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zone aggregation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. At first, you import all the necessary packages and files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') # hide warnings\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Short overview on the next steps given below!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please be aware that the numbering starts at 2 to be compliant with the notebook numbering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OVERVIEW:\n",
    "# Step 2: Relabel the zonenumbers to our actual numbers (such that zone 3 has id 3, not id 7 for example)\n",
    "# Step 3: Split the shapefile into the different regions that will experience different degrees of aggregations\n",
    "# Step 4: Clustering + retrieving for each original zone to which cluster it belongs\n",
    "# Step 5: Update the centroids and the OD-matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Relabel the zonenumbers to the actual numbers (such that zone 3 has id 3, not id 7 for example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2\n",
    "city = \"BRUSSEL\"\n",
    "radius = \"40\"\n",
    "\n",
    "path_orig_shapefile = f\"data_map/STA/shapefiles/{city}_{radius}_10/{city}_{radius}_10.shp\"\n",
    "qgis_path = f\"data_map/QGIS/{city}_{radius}_10.shp\"\n",
    "shapefile = gpd.read_file(path_orig_shapefile)\n",
    "shapefile[\"ZONENUMMER\"] = list(range(1,len(shapefile)+1))\n",
    "shapefile.to_file(qgis_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. This step is done in QGIS. Split the shapefile into the different regions that will experience different degrees of aggregations, in the example case this is chosen to be based on different radiusses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: QGIS\n",
    "'''\n",
    "The main idea here is to determine the different areas around the area of interest that will experience a different degree of aggregation.\n",
    "Here, it is decided that:\n",
    "    * a radius of 10km around Brussels remains unclustered\n",
    "    * a radius from 10 to 20 km is aggregated into somewhat larger zones\n",
    "    * a radius from 20 to 40 km is aggregated even more into large zones\n",
    "For this, the entire region is divided into bands around the center.\n",
    "\n",
    "Step-by-step guide for QGIS:\n",
    "    1. Find centroid of entire region of impact \n",
    "        OUR CASE: \n",
    "            toll_optimization/data_map/QGIS/center.shp\n",
    "\n",
    "    2. Create circle with 45km radius around centroid. 45km instead of the actual radius of 40km of the area to make sure the most outer zones fall into the created circle.\n",
    "\n",
    "    3. Split circle into rings with radiuses of 0-10km (circle to be exact), 10-20km and 20-40km. \n",
    "        e.g. by creating smaller circles and taking differences of the right combinations of circles.\n",
    "        OUR CASE:\n",
    "            toll_optimization/data_map/QGIS/radius_10.shp\n",
    "            toll_optimization/data_map/QGIS/10-20.shp\n",
    "            toll_optimization/data_map/QGIS/20-40.shp\n",
    "\n",
    "    4. For each ring, select the zones of the entire region that are located within the ring. \n",
    "        Hint: use the \"select within\" plugin. There the location is unambiguously decided on the centroid of the zone\n",
    "        OUR CASE:\n",
    "            toll_optimization/data_map/QGIS/BRUSSEL_40_10_0-10.shp\n",
    "            toll_optimization/data_map/QGIS/BRUSSEL_40_10_10-20.shp\n",
    "            toll_optimization/data_map/QGIS/BRUSSEL_40_10_20-40.shp\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Clustering step and afterwards also retrieving for each original zone to which cluster it belongs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: clustering\n",
    "\n",
    "# settings\n",
    "radius1 = \"0-10\"\n",
    "radius2 = \"10-20\"\n",
    "radius3 = \"20-40\"\n",
    "# groupSize1 = 1 not needed\n",
    "groupSize2 = 4\n",
    "groupSize3 = 16\n",
    "\n",
    "# TODO make code more waterproof (naming of input and outputfile)\n",
    "def aggregate_zones(city = city, radius = radius2, idealNbZonesPerCluster = groupSize2):\n",
    "    '''\n",
    "    original shapefile will change: 'cluster' column containing the cluster to which each zone belongs\n",
    "    new shapefill will be created: shapefile consisting of aggregated zones\n",
    "\n",
    "    Note:\n",
    "    code not waterproof: always looks in folder \"QGIS\"\n",
    "    '''\n",
    "    inputFile = f\"data_map/QGIS/{city}_40_10_{radius}.shp\"\n",
    "    outputFile = f\"data_map/QGIS/{city}_40_10_{radius}_knn.shp\"\n",
    "    shapefile = gpd.read_file(inputFile)\n",
    "\n",
    "    points = np.array(shapefile.centroid.apply(lambda p: [p.x, p.y]).tolist())\n",
    "\n",
    "    k =  int(np.ceil(len(shapefile) / idealNbZonesPerCluster))  # number of clusters to create\n",
    "    print(f'Aggregation of {len(shapefile)} zones into {k} clusters.')\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0).fit(points) \n",
    "\n",
    "    shapefile[\"cluster\"] = kmeans.labels_\n",
    "    aggregated = shapefile.dissolve(by=\"cluster\").reset_index()\n",
    "    basicStatisticsZoneSizes = shapefile.groupby('cluster').size().describe() # contains\n",
    "    print(f\"Basic statistics of zone aggregation:\\n{basicStatisticsZoneSizes}\\n\")\n",
    "\n",
    "    # Create a new attribute column to store which elements each cluster contains\n",
    "    aggregated[\"elements_1\"] = \"\"\n",
    "    aggregated[\"elements_2\"] = \"\"\n",
    "\n",
    "    # Retrieve which zones are clustered into which cluster\n",
    "    cluster_groups = shapefile.groupby(\"cluster\")\n",
    "    cluster_elements = {}\n",
    "    for cluster_id, group in cluster_groups:\n",
    "        element_ids = group[\"ZONENUMMER\"].tolist()\n",
    "        cluster_elements[cluster_id] = element_ids\n",
    "\n",
    "    # Update the attribute column with the zonenummers for each cluster. Since the attribute columns are limited in size, \n",
    "    # a second column is used if more than 40 zones are clustered into one aggregated zone. \n",
    "    for index, row in aggregated.iterrows():\n",
    "        cluster_id = row[\"cluster\"]\n",
    "        element_ids = cluster_elements.get(cluster_id, [])\n",
    "        if len(element_ids) > 40:\n",
    "            element_ids1 = element_ids[0:40]\n",
    "            element_ids2 = element_ids[40:]\n",
    "            aggregated.at[index, \"elements_1\"] = \", \".join(str(e) for e in element_ids1)\n",
    "            aggregated.at[index, \"elements_2\"] = \", \".join(str(e) for e in element_ids2)\n",
    "        else: \n",
    "            aggregated.at[index, \"elements_1\"] = \", \".join(str(e) for e in element_ids)\n",
    "    \n",
    "    # Save the new shapefile to a file & update the original file\n",
    "    shapefile.to_file(inputFile)\n",
    "    aggregated.to_file(outputFile)\n",
    "\n",
    "    # Return aggregated shapefile\n",
    "    return aggregated\n",
    "\n",
    "agg_shapefile1 = aggregate_zones()\n",
    "agg_shapefile2 = aggregate_zones(radius = radius3, idealNbZonesPerCluster = groupSize3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Update the centroids and the OD-matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5: Update centroids & OD. First we do some processing to make things easier. \n",
    "\n",
    "city = \"BRUSSEL\"\n",
    "radius1 = \"0-10\"\n",
    "radius2 = \"10-20\"\n",
    "radius3 = \"20-40\"\n",
    "\n",
    "# Quick fix: add 'cluster' column to unclustered 0-10 radius such that later we have a cluster value for all zones in radius 0-40\n",
    "# no k-nearest neighbour is ran on the smallest radius, but just for naming convention some processing is done \n",
    "shapefile0 = gpd.read_file(f'data_map/QGIS/{city}_40_10_{radius1}.shp')\n",
    "shapefile0['cluster'] = range(shapefile0.shape[0])\n",
    "shapefile0['elements_1'] = shapefile0['ZONENUMMER']\n",
    "shapefile0.to_file(f'data_map/QGIS/{city}_40_10_{radius1}_knn.shp') \n",
    "\n",
    "# Concatenate shapefiles\n",
    "shapefile1 = gpd.read_file(f'data_map/QGIS/{city}_40_10_{radius1}_knn.shp')\n",
    "shapefile2 = gpd.read_file(f'data_map/QGIS/{city}_40_10_{radius2}_knn.shp')\n",
    "shapefile3 = gpd.read_file(f'data_map/QGIS/{city}_40_10_{radius3}_knn.shp')\n",
    "\n",
    "# add scalar to cluster numbers before concatenating shapefiles into combined shapefile (otherwise we start counting from 0 multiple times in the combined shapefile)\n",
    "shapefile2['cluster'] = shapefile2['cluster'] + (shapefile1.cluster.max()+1)\n",
    "shapefile3['cluster'] = shapefile3['cluster'] + (shapefile2.cluster.max()+1)\n",
    "\n",
    "# Save combined shapefile\n",
    "combined_shapefile = gpd.GeoDataFrame(pd.concat([shapefile1, shapefile2, shapefile3]))\n",
    "combined_shapefile.to_file(f'data_map/QGIS/{city}_40_10_aggr_comb.shp')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`The final zoning shapefile, containing the aggregated shapefiles all combined, is thus available at: toll_optimization/data_map/QGIS/BRUSSEL_40_10_aggr_comb.shp`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Retrieve original OD matrix, centroids and aggregated shapefile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_od_matrix = pd.read_excel(f\"data_map/STA/OD_matrix/{city}_40_9_.xlsx\") # TODO define dynamically\n",
    "combined_shapefile = gpd.read_file(f'data_map/QGIS/{city}_40_10_aggr_comb.shp')\n",
    "original_shapefile = gpd.read_file(f'data_map/QGIS/{city}_40_10.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize new OD\n",
    "nbAggZones = len(combined_shapefile)\n",
    "aggregated_od_matrix = np.zeros((nbAggZones, nbAggZones))\n",
    "\n",
    "# Retrieve combined zones of each cluster \n",
    "zones_per_cluster = []\n",
    "for index, row in combined_shapefile.iterrows():\n",
    "    zones_cluster_str = row['elements_1']\n",
    "    zones_cluster = [int(e) for e in zones_cluster_str.split(\",\")]\n",
    "    if len(zones_cluster) >= 40:\n",
    "        zones_cluster_str = row['elements_2']\n",
    "        more_zones = [int(e) for e in zones_cluster_str.split(\",\")]\n",
    "        for zone in more_zones:\n",
    "            zones_cluster.append(zone)\n",
    "    zones_per_cluster.append(zones_cluster) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified OD matrix\n",
    "# Outer loop\n",
    "for index_i, row_i in combined_shapefile.iterrows():\n",
    "    # Retrieve all the zones belonging to cluster i    \n",
    "    zones_cluster_i = zones_per_cluster[index_i]\n",
    "    # Inner loop\n",
    "    for index_j, row_j in combined_shapefile.iterrows():\n",
    "        # Retrieve all the zones belonging to cluster j \n",
    "        zones_cluster_j = zones_per_cluster[index_j]\n",
    "        \n",
    "        # Calculate OD flows: sum up the individual flows\n",
    "        aggr_od_flow = 0\n",
    "        for zone_i in zones_cluster_i:\n",
    "            for zone_j in zones_cluster_j:\n",
    "                # VERY IMPORTANT: PANDAS INDEXING SHOULD FIRST SPECIFY COLUMN, AND THEN THE ROW\n",
    "                # Otherwise we are flipping the direction of the summed flows... \n",
    "                aggr_od_flow += original_od_matrix[zone_j-1][zone_i-1]\n",
    "        aggregated_od_matrix[index_i][index_j] = aggr_od_flow\n",
    "\n",
    "# Remove zones that have no demand at all (internal demand is ignored) from the shapefile and the OD_matrix. \n",
    "indices_zero_rows = np.where((aggregated_od_matrix==0).all(axis=1))[0]\n",
    "indices_zero_cols = np.where((aggregated_od_matrix==0).all(axis=0))[0]\n",
    "aggregated_od_matrix = np.delete(np.delete(aggregated_od_matrix, indices_zero_cols, axis=0), indices_zero_rows, axis=1)\n",
    "aggregated_od_matrix = pd.DataFrame(aggregated_od_matrix)\n",
    "combined_shapefile.drop(indices_zero_rows, inplace=True) # Remove entries without creating new geodataframe\n",
    "combined_shapefile[\"cluster\"] = list(range(0,len(combined_shapefile)))\n",
    "\n",
    "# Check: no rows or columns are completely filled with zeroes. --> Prints should be empty\n",
    "indices_zero_rows = np.where((aggregated_od_matrix==0).all(axis=1))[0]\n",
    "indices_zero_cols = np.where((aggregated_od_matrix==0).all(axis=0))[0]\n",
    "print(indices_zero_cols)\n",
    "print(indices_zero_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified centroids\n",
    "import typing\n",
    "from shapely.geometry import Polygon\n",
    "polygons: typing.List[Polygon] = combined_shapefile[\"geometry\"]\n",
    "cluster_ids = combined_shapefile[\"cluster\"]\n",
    "\n",
    "points = [polygon.centroid for polygon in polygons]\n",
    "centroids = []\n",
    "centroid_x = []\n",
    "centroid_y = []\n",
    "for cluster, point in zip(cluster_ids, points):\n",
    "    centroids.append({'cluster_id': cluster, 'X_LAMB': point.x, 'Y_LAMB': point.y})\n",
    "    centroid_x.append(point.x)\n",
    "    centroid_y.append(point.y)\n",
    "combined_shapefile[\"X_LAMB\"] = centroid_x\n",
    "combined_shapefile[\"Y_LAMB\"] = centroid_y\n",
    "centroids_shapefile = gpd.GeoDataFrame(centroids, geometry=points)\n",
    "centroids_shapefile.crs = {'init': combined_shapefile.crs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update centroids in the shapefile\n",
    "combined_shapefile.to_file(f'data_map/QGIS/{city}_40_10_aggr_comb.shp')\n",
    "# Save the centroids to a shapefile, such that they can be added as a layer\n",
    "centroids_shapefile.to_file(f'data_map/QGIS/centroids_BRUSSEL_40_aggr_comb.shp')\n",
    "# Save new OD flows to excel\n",
    "aggregated_od_matrix.to_excel(\"data_map/STA/OD_matrix/BRUSSEL_40_9_aggregated.xlsx\", index=False) # Index is False removes the index column (which matches the format of the original OD matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Congratulations, you have aggregated small zones into larger ones and are set to start assignments on bigger networks!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**** \n",
    "\n",
    "## Elasticity matrix\n",
    "\n",
    "An elasticity matrix between all the aggregated zones is constructed. Elasticities value deviate a standard elasticity, found in literature.\n",
    "\n",
    "The deviations depend on the slices in which zones are situated."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Group zones based on the aggregated slices made in QGIS. In this case specifically, there is one central zone and 6 'pie slices' around it. This is to make a difference between the actual zone of interest (slice 0) and all others providing demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_shapefile = gpd.read_file(f'data_map/QGIS/BRUSSEL_40_10_aggr_comb.shp')\n",
    "# split shapefile into pie slices:\n",
    "slice_0 = gpd.read_file(f'data_map/QGIS/slices/slice_0.shp')\n",
    "slice_1 = gpd.read_file(f'data_map/QGIS/slices/slice_1.shp')\n",
    "slice_2 = gpd.read_file(f'data_map/QGIS/slices/slice_2.shp')\n",
    "slice_3 = gpd.read_file(f'data_map/QGIS/slices/slice_3.shp')\n",
    "slice_4 = gpd.read_file(f'data_map/QGIS/slices/slice_4.shp')\n",
    "slice_5 = gpd.read_file(f'data_map/QGIS/slices/slice_5.shp')\n",
    "slice_6 = gpd.read_file(f'data_map/QGIS/slices/slice_6.shp')\n",
    "slices = [slice_0, slice_1, slice_2, slice_3, slice_4, slice_5, slice_6]\n",
    "\n",
    "# initialize slice list\n",
    "slice_list = [0 for i in range(len(combined_shapefile))]\n",
    "i = 1\n",
    "\n",
    "for slice in slices[1:]:\n",
    "    clusters = slice['cluster'].values\n",
    "    for cluster in combined_shapefile['cluster']:\n",
    "        if cluster in clusters:\n",
    "            print(cluster)\n",
    "            slice_list[cluster] = i\n",
    "    i += 1\n",
    "\n",
    "combined_shapefile['slice'] = slice_list\n",
    "combined_shapefile.to_file(f'data_map/QGIS/BRUSSEL_40_10_aggr_comb.shp')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Initialise elasticity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize elasticity value matrix with ones \n",
    "standard_elasticity = 1\n",
    "\n",
    "standard_elasticity_matrix = [[standard_elasticity for i in range(7)] for j in range(7)]\n",
    "standard_elasticity_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Index all values in the elasticity matrix based on relations between slices (elasticities found in literature, these should normally be thoroughly researched for specific use-cases). Be aware that this is at the moment hardcoded, but it can be improved to have a more flexible style later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexing: standard_elasticity_matrix[row][column]\n",
    "# e.g. standard_elasticity_matrix[0][4] means row 0, column 4. Interpretation: from slice 0 to slice 4\n",
    "\n",
    "to_own = 0.18\n",
    "to_neighbour = 0.18\n",
    "to_others = 0.15\n",
    "to_opposite = 0.15\n",
    "to_Brussels = 0.22\n",
    "from_Brussels = 0.22\n",
    "in_Brussels = 0.25\n",
    "\n",
    "\n",
    "# from 0 to 0\n",
    "standard_elasticity_matrix[0][0] *= in_Brussels\n",
    "\n",
    "# from i to i\n",
    "# from i to Brussels\n",
    "# from Brussels to i\n",
    "for i in range(1,7):\n",
    "    standard_elasticity_matrix[i][i] *= to_own\n",
    "    standard_elasticity_matrix[0][i] *= from_Brussels\n",
    "    standard_elasticity_matrix[i][0] *= to_Brussels\n",
    "\n",
    "\n",
    "# from 1 to neighbouring slices\n",
    "standard_elasticity_matrix[1][2] *= to_neighbour\n",
    "standard_elasticity_matrix[1][6] *= to_neighbour\n",
    "# from 1 to opposite slice\n",
    "standard_elasticity_matrix[1][4] *= to_opposite\n",
    "# from 1 to other slices\n",
    "standard_elasticity_matrix[1][3] *= to_others\n",
    "standard_elasticity_matrix[1][5] *= to_others\n",
    "\n",
    "\n",
    "# from 2 to neighbouring slices\n",
    "standard_elasticity_matrix[2][1] *= to_neighbour\n",
    "standard_elasticity_matrix[2][3] *= to_neighbour\n",
    "# from 2 to opposite slice\n",
    "standard_elasticity_matrix[2][5] *= to_opposite\n",
    "# from 2 to other slices\n",
    "standard_elasticity_matrix[2][4] *= to_others\n",
    "standard_elasticity_matrix[2][6] *= to_others\n",
    "\n",
    "\n",
    "# from 3 to neighbouring slices\n",
    "standard_elasticity_matrix[3][2] *= to_neighbour\n",
    "standard_elasticity_matrix[3][4] *= to_neighbour\n",
    "# from 3 to opposite slice\n",
    "standard_elasticity_matrix[3][6] *= to_opposite\n",
    "# from 3 to other slices\n",
    "standard_elasticity_matrix[3][1] *= to_others\n",
    "standard_elasticity_matrix[3][5] *= to_others\n",
    "\n",
    "\n",
    "# from 4 to neighbouring slices\n",
    "standard_elasticity_matrix[4][3] *= to_neighbour\n",
    "standard_elasticity_matrix[4][5] *= to_neighbour\n",
    "# from 4 to opposite slice\n",
    "standard_elasticity_matrix[4][1] *= to_opposite\n",
    "# from 4 to other slices\n",
    "standard_elasticity_matrix[4][2] *= to_others\n",
    "standard_elasticity_matrix[4][6] *= to_others\n",
    "\n",
    "\n",
    "# from 5 to neighbouring slices\n",
    "standard_elasticity_matrix[5][4] *= to_neighbour\n",
    "standard_elasticity_matrix[5][6] *= to_neighbour\n",
    "# from 5 to opposite slice\n",
    "standard_elasticity_matrix[5][2] *= to_opposite\n",
    "# from 5 to other slices\n",
    "standard_elasticity_matrix[5][1] *= to_others\n",
    "standard_elasticity_matrix[5][3] *= to_others\n",
    "\n",
    "# from 6 to neighbouring slices\n",
    "standard_elasticity_matrix[6][5] *= to_neighbour\n",
    "standard_elasticity_matrix[6][1] *= to_neighbour\n",
    "# from 6 to opposite slice\n",
    "standard_elasticity_matrix[6][3] *= to_opposite\n",
    "# from 6 to other slices\n",
    "standard_elasticity_matrix[6][2] *= to_others\n",
    "standard_elasticity_matrix[6][4] *= to_others\n",
    "\n",
    "# round results up to 3 \n",
    "slice_elasticity_matrix =  [[round(standard_elasticity_matrix[i][j], 3) for i in range(7)] for j in range(7)]\n",
    "\n",
    "slice_elasticity_matrix\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Make a zone elasticity matrix (600x600) using the slice elasticity matrix (7x7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_shapefile = gpd.read_file(f'data_map/QGIS/BRUSSEL_40_10_aggr_comb.shp')\n",
    "zone_elasticity_matrix = np.zeros((len(combined_shapefile), len(combined_shapefile)))\n",
    "\n",
    "# Loop through all pairs of observations and fill in the matrix with the corresponding slice value\n",
    "for i in range(len(combined_shapefile)):\n",
    "    for j in range(len(combined_shapefile)):\n",
    "        k = combined_shapefile.iloc[i]['slice']\n",
    "        l = combined_shapefile.iloc[j]['slice']\n",
    "        zone_elasticity_matrix[i][j] = slice_elasticity_matrix[k][l]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Determine the useful path to the elasticity matrix. This is for ease of use later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elasticity_path = f\"data_map/STA/elasticity/Brussel_40\"\n",
    "np.savetxt(elasticity_path,zone_elasticity_matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Congratulations, you now have made your elasticity matrix B as a building block to perform elastic assignments!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dyntapy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
